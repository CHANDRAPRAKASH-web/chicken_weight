
# train_mlp.py
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from torch.utils.data import TensorDataset, DataLoader
from pathlib import Path
import argparse
import json
import random

parser = argparse.ArgumentParser()
parser.add_argument('--features', default='dataset/features/features.npy')
parser.add_argument('--meta', default='dataset/crops/crops_meta.csv')
parser.add_argument('--weights', default='weights.csv')   # path to your weights.csv (orig image -> weight)
parser.add_argument('--out', default='models', help='save dir')
parser.add_argument('--epochs', type=int, default=50)
parser.add_argument('--batch', type=int, default=32)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--seed', type=int, default=42)
args = parser.parse_args()

# reproducibility
torch.manual_seed(args.seed)
np.random.seed(args.seed)
random.seed(args.seed)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device:', device)

# load features and meta
X = np.load(args.features)    # shape N x D
crops_meta = pd.read_csv(args.meta)   # must have crop_filename, orig_image columns
weights_df = pd.read_csv(args.weights) # columns: filename,weight_g

# normalize names to string
weights_df['filename'] = weights_df['filename'].astype(str)

# mapping orig_image -> weight
wmap = dict(zip(weights_df['filename'], weights_df['weight_g']))

# build target array y aligned with crops_meta order
y = []
missing = 0
for i, r in crops_meta.iterrows():
    orig = r.get('orig_image')
    if orig in wmap:
        y.append(float(wmap[orig]))
    else:
        y.append(np.nan)
        missing += 1

y = np.array(y, dtype=np.float32)
print('Total crops:', len(y), 'missing weights:', missing)

# drop rows with no weight (we cannot supervise these crops)
mask = ~np.isnan(y)
X = X[mask]
y = y[mask]

print('Final samples after dropping missing:', X.shape[0])

# Normalize targets (mean/std) and save scaler later
y_mean = float(y.mean())
y_std  = float(y.std()) if float(y.std())>0 else 1.0
y_norm = (y - y_mean) / y_std

print('y mean, std:', y_mean, y_std)

# split
X_train, X_val, y_train, y_val = train_test_split(X, y_norm, test_size=0.2, random_state=args.seed)

# torch loaders
train_ds = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))
val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))
train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False)

# MLP model
class MLP(nn.Module):
    def __init__(self, in_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 256),
            nn.ReLU(),
            nn.Linear(256,128),
            nn.ReLU(),
            nn.Linear(128,1)
        )
    def forward(self,x):
        return self.net(x).squeeze(1)

model = MLP(in_dim=X.shape[1]).to(device)
opt = torch.optim.Adam(model.parameters(), lr=args.lr)
loss_fn = nn.MSELoss()

Path(args.out).mkdir(parents=True, exist_ok=True)
best_val = 1e12

for epoch in range(1, args.epochs+1):
    model.train()
    train_loss = 0.0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        pred = model(xb)
        loss = loss_fn(pred, yb)
        opt.zero_grad(); loss.backward(); opt.step()
        train_loss += loss.item() * xb.size(0)
    train_loss /= len(train_loader.dataset)

    # val
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            pred = model(xb)
            val_loss += loss_fn(pred, yb).item() * xb.size(0)
    val_loss /= len(val_loader.dataset)

    print(f'Epoch {epoch} train_loss={train_loss:.4f} val_loss={val_loss:.4f}')

    if val_loss < best_val:
        best_val = val_loss
        torch.save(model.state_dict(), Path(args.out)/'mlp.pth')
        print('Saved best model ->', Path(args.out)/'mlp.pth')

# Save scaler (mean/std) and meta mapping
scaler_path = Path(args.out)/'scaler.json'
with open(scaler_path, 'w') as f:
    json.dump({'y_mean': y_mean, 'y_std': y_std}, f)
print('Saved scaler ->', scaler_path)
print('Done. Best val MSE:', best_val)